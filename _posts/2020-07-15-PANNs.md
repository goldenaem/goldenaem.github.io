---
title: "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition"
date: 2020-07-15 00:06:28 -0400
categories: spectrogram wave speech 표상학습 음성 오디오 pre-train
---

해당 포스트에서 정리할 논문은 [PANNs](https://arxiv.org/pdf/1912.10211.pdf)다. (2019년 12월 21일자 논문)

transfer learning의 일종으로 매우 큰 dataset([AudioSet](https://ieeexplore.ieee.org/abstract/document/7952261))으로 audio tagging을 학습한 후에, fine-tunning을 통해 acoustic scene classification, general audio tagging, music classification, speech emotion classification 등의 task에서 좋은 성능을 보였다고 한다.

최근 pre-train은 대체로 self-supervised, 혹은 unsupervised인 경우가 많은데 이 논문 같은 경우 AudioSet이라는 label된 대용량 데이터를 사용해서 PANNs을 학습하고, 데이터가 적은 fine-tunning task에 적용하는 방향으로 실험하였다. 

논문의 아이디어는 매우 간단하고 Vision와 NLP 영역과 비교하면 오히려 label된 데이터를 사용하여 pre-train한다는 측면에서 실제 현장에 적용하기 어려운 한계도 보이지만, 그동안 audio 데이터를 다루었던 여러 논문들과의 차이점은 입력 정보로 wave와 (log)-mel-spectrogram 2가지 표현법을 동시에 사용한 부분이다.~~(PANNs이 2가지 표현법을 동시에 사용하는 최초의 논문인지는 확실치 않다.)~~

논문에 따르면, 기존 연구들에서 wave의 1-dimensional CNN 보다 log-mel-spectrogram를 입력으로 사용하는 2-dimensional 모델이 더 성능이 좋았고, 이러한 점은 wave같은 time-domain은 frequency정보를 capture하지 못하기 때문이라고 한다.(이는 wave정보를 처리하는 것은 frequency shift의 invariant하지 못하기 때문이라고 한다.)

이를 해소하고자 Wavegram-CNN과 Logmel-CNN을 concat해서 사용하는 아래와 같은 모델 구조를 제시한다.

![wavegram-logmel-CNN](/assets/images/wavegram-logmel-CNN.jpg)

wave는 1-D CNN을 통해, wave를 log-mel-spectrogram으로 변환한 후에 (변환 과정은 [링크](https://goldenaem.github.io/signal_processing/spectrogram/mel-spectrogram/stft/fourier/wave/preprocessing/Signal_Processing-preprocessing/)참고) 2-D CNN으로 feature를 뽑아내고 그 둘을 concat하여 prediction에 사용하는 것이다. Wavegram의 마지막 layer인 reshape는 (batch_size, # of frames, freq_bin, channel/freq_bin)으로 하여 log-mel-spectrogram의 Conv2D block 결과와 사이즈를 맞추는 듯 하다.

concat한 feature로 label을 prediction 하기 전에 2D CNN layers는 논문에서 정리해둔 CNN계열의 여러 모델들을 사용하여 진행함. 또한 fixed length time data를 사용하거나 crop하지않고, 2D CNN layer의 feature 추출 이후에 time-axis에 대하여 global pooling을 적용하여 Linaer(fully-connected) layer를 거쳐 label을 prediction한다. 이때 N개의 label 각각에 binary cross entropy를 사용했다고 한다.
$$ l = - \sum_{n=1}^{N} (y_n \ln f(x_n) + (1-y_n) \ln(1-f(x_n)) $$
pre-train에 사용한 AudioSet의 label이 527개이기에 여기서 N은 527이고, 이후 concat된 결과를 feature로 사용하여 fine-tunning 하는 방식으로 transfer learning한다. 

Data balancing(AudioSet의 클래스 분포가 long-tail distribution이므로 batch내의 클래스를 uniform하게 선택하고, 선택된 class별로 audio clip을 sample함)과 Data augmentation(Mix-up과 SpecAugment, 추후에 정리할 예정)을 사용함. 

AudioSet에 대한 기존 연구보다 성능이 높으며, fine-tunning task에서도 좋은 성능을 보였다고 한다. 

마지막으로 github에 공개된 pytorch [코드](https://github.com/qiuqiangkong/audioset_tagging_cnn)가 파악하기 간단하여 정리하고자 한다. 다만, 세세하게 파고들면 분량이 너무 길어지므로 메인 흐름만 파악할 수 있도록 정리하려함.

[main.py](https://github.com/qiuqiangkong/audioset_tagging_cnn/blob/master/pytorch/main.py)를 보면 argument, parameter, configuration, path, model_save, logger, evaluate 등을 제외하면 main(train)내에 순수 학습에 관여하는 코드는 많지 않다. 이해를 위한 코드이므로 가정과 생략이 많이 포함됨.

	def train(args): # 58
		"""
		생략. 각 line별 주석은 실제 main code에서의 line number
		"""
		Model = eval(model_type) #151
		model = Model(sample_rate=sample_rate, window_size=window_size, hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, classes_num=classes_num) # 152
		dataset = AudioSetDataset(clip_samples=clip_samples, classes_num=classes_num) # 163
		Sampler = TrainSampler #167, data balancing 사용 안한다고 가정
		train_sampler = Sampler(indexes_hdf5_path=train_indexes_hdf5_path, batch_size=batch_size, black_list_csv=black_list_csv) # 173, mix-up data augmentation 사용 안한다고 가정
		train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_sampler=train_sampler, collate_fn=collate_fn, num_workers=num_workers, pin_memory=True) # 186
		optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0., amsgrad=True) # 208
		iteration = 0 # 230
		model.to(device) # 238
		for batch_data_dict in train_loader: # 242
			for key in batch_data_dict.keys(): #298
	            batch_data_dict[key] = move_data_to_device(batch_data_dict[key], device) #299
	        model.train() #302
	        batch_output_dict = model(batch_data_dict['waveform'], None) # 313
	        batch_target_dict = {'target': batch_data_dict['target']}   # 316
	        loss = loss_func(batch_output_dict, batch_target_dict) # 320
	        loss.backward() # 323
	        optimizer.step() #326
        	optimizer.zero_grad() #327

여기서 AudioSetDataSet, TrainSampler, dataloader는 [링크](https://goldenaem.github.io/signal_processing/spectrogram/mel-spectrogram/stft/fourier/wave/preprocessing/Signal_Processing-preprocessing/)를 참고. 이렇게 정리하고 나면 결국은 model만 위의 train 코드와 맞춰서 설정해주면 끝이다.(입력, 출력의 형태만 맞추면 됨.)

[models.py](https://github.com/qiuqiangkong/audioset_tagging_cnn/blob/master/pytorch/models.py)를 살펴보면 여러 모델들을 정의해 두었다. data(input : [batch_size, ...] )과 mixup_lambda(data augmention 관련)를 입력으로, class_predict(clipwise_output : [batch_size, class_num])과 extracted feature(embedding)을 element로 가지는 dictionary를 출력으로 해놓고 원하는 대로 모델을 정의하여 사용가능하다. 원하는 모델을 미리 정의해 놓고 train 함수에서는 해당 string을 eval하는 방법으로 모델 인스턴스를 생성하여 학습에 사용한다.(# 151줄 참고) 아래는 이해를 위해 간단한 형태로 모델을 define해 보았다.(train과 Cnn 모두 예제임을 다시 한번 강조한다.)

	class Cnn(nn.Module):
		def __init__(self, *args, **kwargs):
			super(Cnn, self).__init__()
			self.conv_block = ConvBlock5x5(in_channels=1, out_channel=512)
			self.fc = nn.Linear(512, classes_num, bias=True)

		def forward(self, input, mixup_lamb=None):
			x = self.conv_block(x, pool_size=(2,2), pool_type='avg')
			x = torch.mean(x, dim=3) # global pooling
			(x1, _) = torch.max(x, dim=2)
			x2 = torch.mean(x, dim=2)
			x = x1 + x2
			embedding = x
			clipwise_output = torch.sigmoid(self, self.fc(x))
			output_dict = {'clipwise_output' : clipwise_output, 'embedding' : embedding}
			return output_dict


* 모델을 미리 define하지 않고 argument만으로 새로운 구조를 만드는 코드는 추후에 정리할 예정 *


***
  reference)

  Kong, Qiuqiang, et al. "Panns: Large-scale pretrained audio neural networks for audio pattern recognition." arXiv preprint arXiv:1912.10211 (2019).
  https://github.com/qiuqiangkong/audioset_tagging_cnn
  Gemmeke, Jort F., et al. "Audio set: An ontology and human-labeled dataset for audio events." 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017.

