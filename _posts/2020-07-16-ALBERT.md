---
title: "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
date: 2020-07-16 00:06:28 -0400
categories: 표상학습 pre-train self-supervised NLP BERT ALBERT compression parameter-sharing 
use_math : true
---

해당 포스트에서 정리할 논문은 [ALBERT](https://arxiv.org/abs/1909.11942)다. (2019년 9월 26일자 논문)

기존 BERT 모델에 3가지를 추가/변경하여 BERT의 memory 사용량과 training 시간 감소시킴.(BERT 추후에 정리할 예정)

첫번째로 BERT는 Word embedding을 사용하여 token을 hidden dimension($$H$$)과 동일하게 유지하지만, ALBERT는 초기 word embedding에는 hidden 만큼의 dimension이 필요하지 않으므로 embedding dimension($$E, H >> E $$)를 사용하고, embedding dimension vector를 hidden dimension vector로 project하여 hidden vector를 구성하는 방법을 제시함. 기존에 vocab embedding matrix ($$V \times H$$)를 vocab embedding matrix($$V \times E$$)와 projection($$E \times H$$)로 factorize함. 이 경우 V는 약 30000, H는  768/1024/2048/4096이고 E는 128을 사용하였다. 

두번째로 BERT에서 각 Layer마다 개별적인 multi-head self-attetion과 feed-forward network를 사용하는데, ALBERT는 이를 하나의 동일한 layer로 sharing하여 recursive하게 layer를 통과하도록 제안한다. 이 cross-layer parameter sharing를 사용하면 model의 memory를 절약할 수 있으나 layer의 갯수가 동일하다면 연산량은 차이나지 않을 것이다.

![ALBERT compress](/assets/images/ALBERT_compression.JPG)

위의 표에서 알 수 있듯이 parameter수는 몇 배로 줄었으나 동일 크기의 학습 속도 향상은 그렇게 크지 않다.(1.7배 혹은 5.6/4.7 = 1.19배) 

논문에서 주장하는 cross-layer parameter sharing의 다른 장점은 network가 안정화 된다고 한다.

![ALBERT layer dist](/assets/images/ALBERT_layer_dist.JPG)

위 그림은 각 layer별로 layer의 입력 embedding과 출력 embedding의 L2 Distance, Cosine Similarity를 계산한 그래프이다. 이를 통해 논문에서는 기존 BERT에 비해 layer에서 layer로의 transition이 smooth되었고, ALBERT가 모델의 parameter를 안정화시켰다고 주장한다.

세번째는 inter-coherence loss를 NSP(Next Sentence Prediction)에서 SOP(Sentence Order Prediction)으로 변경하였다. BERT의 NSP task가 주어진 두 문장이 자연스러운지를 파악하기 보다 topic(batch혹은 corpus에서 negative sample을 수행하므로)을 파악하는 정도이기 때문에 적합하지 않다고 주장하며 SOP task를 대안으로 제시한다. SOP는 negative sample을 동일 batch/corpus에서 생성하지 않고, 동일 sample내에서 생성하여 문장의 순서가 올바른지를 판단하도록 학습한다.

![ALBERT_SOP](/assets/images/ALBERT_SOP.JPG)

논문의 실험 결과는 위의 표와 같았다. Sentence Prediction(SP, inter-coherence loss) task를 수행하지 않은 결과(None)와 NSP, SOP를 비교하면 NSP로 학습된 모델은 NSP task를 잘하지만, 문장의 순서를 맞추는 SOP task에서는 안 좋은 성능을 보였다.(binary classification에서 52%는 random과 유사한 결과) 그에 반해 SOP로 학습된 결과는 SOP뿐만 아니라, NSP task에서도 적절한 성능을 보임을 알 수 있다. 그외에 SP task를 따로 학습하지 않은 경우에서도 down-stream task의 성능이 NSP와 크게 다르지 않음을 보이며 기존 BERT의 NSP task의 한계를 주장한다. 

그 외에 Masking은 n-gram(3-gram)을 사용하였고, Optimizer로 LAMB을 사용하였다.(LAMB 추후에 정리할 예정) 또한 모델의 수렴 여부는 pre-train task의 accuracy로 판단하였고, 기존 BERT의 dropout을 없애는 것이 MLM accuracy를 크게 향상시켰다고 한다.(CNN에서 dropout과 batchnorm을 같이 사용하면 안좋다는 [이론적](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.html), [실험적](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPaper/14806) 연구가 있었다고 한다.) pre-train의 성능이 높아지면 fine-tunning task의 성능도 같이 높아지는 경향을 보아서는 overfitting은 안된다고 주장한다.(pre-train 성능이 높아졌는데 fine-tunning task 성능이 낮아지는 경우가 없었다는 의미)

마지막으로 pytorch 코드를 분석한 내용을 정리하며 포스팅을 마무리 지으려 한다.

코드는 pytorch의 [BERT](https://github.com/dhlee347/pytorchic-bert)를 참고한 [ALBERT](https://github.com/graykode/ALBERT-Pytorch)를 바탕으로 이해를 돕기위해 [링크](https://github.com/goldenaem/ALBERT-Pytorch)에서 재구성 하였다. (지금은 이해를 돕기 위한 코드 수준으로 에러 발생 가능. 추후에 학습이 에러없이 작동하도록 업데이트할 예정)

제일 중요한 부분은 pre-train이므로(evaluate, fine-tunning, logger, save/load, ... 등은 pre-train을 이해하면 추가하여 구성할 수 있는 부분이므로) [pretrain.py](https://github.com/graykode/ALBERT-Pytorch/blob/master/pretrain.py)부터 살펴보면,

![pretrain-main](/assets/images/ALBERT_SOP.JPG)



 모델을 define한 [models.py](https://github.com/graykode/ALBERT-Pytorch/blob/master/models.py)를 중점적으로 argument, parameter, configuration등을 제외하여 정리함.





    def main(args): # 236
        tokenizer = tokenization.FullTokenizer(vocab_file = args.vocab, do_lower_case = True) # 243
        tokenize = lambda x : tokenizer.tokenize(tokenizer.convert_to_unicode(x)) # 244

        pipeline = [Preprocess4Pretrain(*args)] # 246
        data_iter = SentPairDataLoader(*args) # 254
        model = BertModel4Pretrain(model_cfg) # 260
        criterion_MLM = nn.CrossEntropyLoss(reduction='none')
        criterion_sop = nn.CrossEntropyLoss()
        optimizer = optim.optim4GPU(cfg, model)
        trainer = train.Trainer(cfg, model, data_iter, optimizer, ~)
        trainer.train(get_loss, ~)

        def get_loss(model, batch, global_step):
            input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next = batch
            logits_lm, logits_clsf = model(input_ids, segment_ids, input_mask, masked_pos)
            loss_lm = criterion_MLM(logits_lm, masked_ids)








***
  ref)MLA

  Lan, Zhenzhong, et al. "Albert: A lite bert for self-supervised learning of language representations." arXiv preprint arXiv:1909.11942 (2019).
  Li, Xiang, et al. "Understanding the disharmony between dropout and batch normalization by variance shift." Proceedings of the IEEE conference on computer vision and pattern recognition. 2019.
  Szegedy, Christian, et al. "Inception-v4, inception-resnet and the impact of residual connections on learning." Thirty-first AAAI conference on artificial intelligence. 2017.
  https://github.com/dhlee347/pytorchic-bert
  https://github.com/graykode/ALBERT-Pytorch
  https://www.youtube.com/watch?v=mxGCEWOxfe8