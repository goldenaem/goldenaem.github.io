---
title: "Randomly Weighted CNNs for (Music) Audio Classification"
date: 2020-07-15 00:06:28 -0400
categories: spectrogram wave speech 표상학습 음성 오디오 pre-train
---

해당 포스트에서 정리할 논문은 [Randomly Weighted CNNs for Audio Classification](https://arxiv.org/abs/1805.00237)다. (2018년 5월 1일자 논문)

기존에 Random wieghted CNN을 사용하여 feature를 추출하여 학습을 우회했던 [연구](http://www.robotics.stanford.edu/~ang/papers/nipsdlufl10-RandomWeights.pdf) 바탕으로 Audio classification에도 동일하게 적용한 논문이다. 

Random weighted CNN(학습과정 없이 초기화된 상태의 모델)를 feature extractor로 사용하고 Classifier로 SVM(support vector machine)이나 ELM(extreme learning machines)를 사용하여 classification을 진행한다.

논문에서 제시한 CNN구조는 입력 타입(wave/spectrogram)과 domain-dependency과 filter config에 따라 (domain knowledge-sigle filter / domain knowledge-many filter / not use)에 따라 아래와 같이 크게 6가지로 나뉜다.

![RWCNN-design](/assets/images/RWCNN_design.jpg)

wave와 spectrogram에 대해서는 [링크](https://goldenaem.github.io/signal_processing/spectrogram/mel-spectrogram/stft/fourier/wave/preprocessing/Signal_Processing-preprocessing/) 참고.
domain knowledge는 해당 데이터가 wave/spectrogram이라는 전제하에 해당 데이터를 좀더 의미있게 처리하기 위해 filter size를 조절하는 것을 의미함.
single-filter와 many-filter의 차이는 동일한 1개의 filter만 가지고 feature를 추출하는지, 다양한 크기의 filter들을 가지고 feature를 추출하는지를 나타냄.

따라서 wave를 처리하는 Frame Level은 폭넓은 filter size(512, 256, 128)와 stride(256, 64)을 사용하고, spectrogram는 mel-filterbank feature를 1개의 단위로 vertical하게, frame들의 처리는 horizontal하게 처리한다. domain knowledge를 사용하지 않는 경우는 vision에서 흔히 사용하던 kernel size 3짜리의 1D/2D convolutional layer를 stack하는 방식으로 처리한다.(2D CNN은 VGG사용)

* 참고) 논문에서는 mel-spectrogram의 feature 수를 96으로 설정하였는데, 따라서 $$7 \times 96$$과 $$7 \times 86$$으로 kernel을 설정하여 mel-spectrogram을 처리한다.([96](https://ieeexplore.ieee.org/document/6854950/), [86](https://arxiv.org/abs/1703.06697)) 논문에서는 kernel_size가 $$7 \times 86$$이 $$7 \times 96$$보다 성능이 좋다고 하는데, 그 이유는 shift-invariant하기 때문이라고 언급함. *

그리고 mel-filterbank feature에서 temporal(horizontal)과 timbral(vertical)을 average하여 동시에 고려한 아래와 같은 kernel도 사용한다.

![temporal-timbral](/assets/images/temp_timb.JPG)

모델의 결과를 보면 아래와 같다.

![RWCNN-result](/assets/images/RWCNN_result.JPG)

데이터셋과 feature vector의 dimension에 따라 결과는 다르지만, 초기화된 CNN계열의 모델을 feature exrtactor로 사용하였을 때 성능이 MFCC같은 hand-craft feature와 유사/초과한다고 논문에서 주장한다.

논문에 사용했던 GTZAN같은 데이터 셋의 경우 state-of-the-art가 90%를 가뿐히 넘기 때문에 초기화된 모델을 사용하자는 주장은 아니지만 서로 다른 CNN을 feature extractor로서 사용할 때, CNN 구조를 평가할 cheap한 방법이라고 이야기한다. 그러나 future work로 남겨둔 내용과는 다르게 데이터를 통해 학습을 진행하면, 초기화에서 성능이 높았던 모델이 학습 후에도 유사하게 좋은 feature extractor일지는 데이터셋마다 다를 것이라고 생각한다. 


***
  reference)

  Pons, Jordi, and Xavier Serra. "Randomly weighted CNNs for (music) audio classification." ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2019.
  Saxe, Andrew M., et al. "On random weights and unsupervised feature learning." ICML. Vol. 2. No. 3. 2011.
  Dieleman, Sander, and Benjamin Schrauwen. "End-to-end learning for music audio." 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.
  Pons, Jordi, et al. "Timbre analysis of music audio signals with convolutional neural networks." 2017 25th European Signal Processing Conference (EUSIPCO). IEEE, 2017.


