---
title: "Audio ALBERT : A Lite BERT for Self-supervised Learning of Audio Representation"
date: 2020-07-13 21:26:28 -0400
categories: ALBERT BERT pre-train self-supervised audio speech 표상학습 음성 오디오
use_math : true
---

해당 포스트에서 정리할 논문은 [Audio ALBERT(AALBERT)](https://arxiv.org/pdf/2005.08575.pdf)이다.(2020년 5월 18일자 논문)

NLP 도메인에 주로 사용되었던 BERT류의 모델을 audio, 그중에서 speech representation learning에 적용한 논문이다.

메모리와 연산능력이 매우 많이 필요로 하는 (16TPU로 81h) BERT를 사용하지 않고, ALBERT라는 model compression기법이 적용된 BERT 모델을 사용하였다. 이는 1개의 32GB Tesla GPU로 학습을 시켰다고 한다.(학습 소요시간이 적혀있진 않다.) (BERT, ALBERT 추후에 정리할 예정) ~~(ALBERT의 논문 제목을 완전 오마주한 제목이 인상적)~~

기존 BERT류의 모델들과 다른 점은 아래 그림과 같이 입력의 형태가 discrete된 word embedding(token)이 아니라 continuous한 spectrogram의 값을 사용한다는 것이다.((spectrogram)[https://goldenaem.github.io/signal_processing/spectrogram/mel-spectrogram/stft/fourier/wave/preprocessing/Signal_Processing-preprocessing/] 참고) 

![audio-albert](/assets/images/aalbert.JPG)

왼쪽에 Mockingjay는 AALBERT에서 비교로 삼는 선행 연구이다.(mockingjay 추후에 정리할 예정) 참고로 두 논문은 같은 연구실에서 나온 논문이다. continuous한 spectrogram을 입력으로 하는 BERT를 만들고(mockingjay), 후속으로 ALBERT를 사용하는 AALBERT를 작성함.(소스코드 깃헙도 합쳐진 상태) 

기존 BERT류의 모델은 Masking된 입력에 해당하는 output의 target으로 기존 token의 softmax, 혹은 어떻게든(Selfie, NCELoss를 사용하는 류의 연구들) regression문제를 classification 문제로 바꿔서 풀려는 접근이 많았다.(Selfie, NCELoss 추후에 정리할 예정)

그러나 AALBERT는 Mockingjay에서 처럼 ground-truth 값의 L1 loss, 즉 reconstruction을 수행하도록 하는 방향으로 학습을 시킨다.

*여담으로 wav2vec의 후속 연구인 vq-wav2vec을 보면 feature를 discrete하게 만들어서(quantize) BERT를 사용하려는 방향과는 상당히 대조적이다.(wav2vec, vq-wav2vec 추후에 정리할 예정) 그 외에 다른 논문에서도 Discrete BERT가 Continuous BERT보다 성능이 좋다는 연구(추후에 정리할 예정)가 있다는걸 보면 AALBERT는 Reconstruction을 통해서 학습할 수 도 있다는 방향을 제시한 정도라고 생각한다.*

추가적으로 또 눈에 띄는 내용은 probing task라는 실험이다. 

![aalbert-probing](/assets/images/aalbert_probing.JPG)
ALBERT의 중간 Layer들의 feature들을 바탕으로 fine-tunning을 수행한 결과다.

성능 그래프의 전반적인 결과는 첫번째 layer부터 깊어질수록 성능이 오르다가 마지막 layer에서 성능이 낮아지는 결과를 확인할 수 있다. 저자도 논문에 적어둔 내용이지만, 이는 pre-train task에 과도하게 fitting된 결과라고 해석한다~~(동의함)~~. 아마도 L1 loss로 reconstruction을 한 부작용이 아닐까하고 추측한다.

마지막으로 모델 configuration을 정리하면 입력으로 80 mel_bin의 mel-spectrogram(along with its delta), hidden size는 160, layer는 6, masking은 3frame(3-gram)씩 적용했다고 함.  

***

'추후에 정리할 예정'은 해당 내용을 포스팅하면 포스트 링크로 수정하겠습니다.

***
	ref)

	Chi, Po-Han, et al. "Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio Representation." arXiv preprint arXiv:2005.08575 (2020).



